{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Flowz workflow management primer","text":"<p>Flowz is a code repository templater that provides a development environment for integrating your DAGs into the Apache Airflow workflow management system with minimal fuss. Flowz also provides default tooling for the full software development lifecycle and deployment into a containerised environment.</p>"},{"location":"#wait-what-is-a-dag","title":"Wait, what is a DAG?","text":"<p>As per the Apache Airflow DAG documentation:</p> <p>A DAG (Directed Acyclic Graph) is the core concept of Airflow, collecting Tasks together, organized with dependencies and relationships to say how they should run.</p> <p>Programmatically, a DAG is nothing more than a Python module. As such, consider your workflows as a Python logic whereby software development principles can be applied. Mainly code linting, formatting and tests. Flowz provides a default set of tooling for the SDLC. These are interchangeable to suit your team and projects requirements.</p> <p>Note</p> <p>Flowz does not intend to tell you how you should do things. It's here to help you work common tasks.</p>"},{"location":"#where-do-i-start","title":"Where do I start?","text":""},{"location":"#creating-dags","title":"Creating DAGs","text":"<p>As Flowz is an Apache Airflow DAG code repository templater, begin by forking a copy into your preferred code repository in the first instance. Next, follow the instructions under the Flowz getting started section.</p>"},{"location":"#getting-ready-for-production-building-the-container-image","title":"Getting ready for production: building the container image","text":"<p>Flowz provides the tooling to support a cloud-native deployment model. Follow the: guide under container image management to containerise your workflows so that they are ready for deployment.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>Airflow provides different types of installations that are defined by the Executor type. Flowz supports SequentialExecutor for development and CeleryExecutor for production workloads. The following section details how to setup the development environment and how to create your first DAG.</p> <p>First, prepare your local development environment before creating a DAG from the default Flowz template.</p>"},{"location":"installation-modes/celeryexecutor/architecture/","title":"Architecture","text":"<p>This section features an overview of detailed Apache Airflow Celery Executor architecture.</p> <p>The CeleryExecutor Apache Airflow installation type consist of several components:</p> <ul> <li>Workers: execute the assigned tasks.</li> <li>Scheduler: responsible for adding the necessary tasks to the queue.</li> <li>Web server: HTTP Server provides access to DAG/task status information</li> <li>Database: contains information about the status of tasks, DAGs, Variables, connections, etc. Flowz uses PostgreSQL.</li> <li>Celery: distributed task queue mechanism. Flowz uses Redis as the broker.</li> </ul> <p></p> <p>Apache Airflow in CeleryExecutor mode is fit for production-grade deployments.</p>"},{"location":"installation-modes/celeryexecutor/build/","title":"Build","text":"<p>Containerisation of your workloads is a requirement for the Celery Executor installation type.</p> <p>Note</p> <p>See Makester's <code>docker</code> subsystem for more detailed container image operations.</p> <p>The container image is based on a customised variant of the Apache Airflow project's Dockerfile. The main difference is that the <code>PYTHON_BASE_IMAGE</code> is overridden with Ubuntu as the underlying OS. This provides better currency to mitigate CVEs. The final Flowz Airflow image extends the customised base container image with the project's workflow capability (DAGs, operators and hooks).</p>"},{"location":"installation-modes/celeryexecutor/build/#container-image-validation-in-local-environment","title":"Container image validation in local environment","text":"<p>Local container image build and validation allows you to launch a running instance of Apache Airflow before pushing to the image repository.</p> <p>Note</p> <p>Flowz embeds your workflow capability into the Flowz container image. As such, modifications to the workflow logic will require a fresh container image build.</p>"},{"location":"installation-modes/celeryexecutor/build/#build-the-container-image-for-local-testing","title":"Build the container image for local testing","text":"<pre><code>make local-image-buildx\n</code></pre> <p>The conatiner image build process will created two tags:</p> <ul> <li>the SemVer taken from <code>src/flowz/VERSION</code></li> <li><code>latest</code></li> </ul> <p>Note</p> <p>See Makester's versioning subsystem on how Flowz maintains release versions.</p>"},{"location":"installation-modes/celeryexecutor/build/#search-for-built-container-image","title":"Search for built container image","text":"<pre><code>make image-search\n</code></pre> <p>Typical output based on SemVer value of <code>2.8.1-0.1.1</code> contained within <code>src/flowz/VERSION</code>: Flowz container image tags.<pre><code>REPOSITORY     TAG           IMAGE ID       CREATED         SIZE\nloum/flowz   2.8.1-0.1.1   bcb2c5443e69   9 minutes ago   1.65GB\nloum/flowz   latest        bcb2c5443e69   9 minutes ago   1.65GB\n</code></pre></p>"},{"location":"installation-modes/celeryexecutor/build/#delete-the-container-image","title":"Delete the container image","text":"<pre><code>make image-rm\n</code></pre>"},{"location":"installation-modes/celeryexecutor/instance-runtime/","title":"Airflow instance runtime","text":"<p>Launch the Airflow webserver UI in Celery Executor mode to visualise and interact with dashboard:</p>"},{"location":"installation-modes/celeryexecutor/instance-runtime/#start","title":"Start","text":"MakesterDocker CLI <pre><code>make celery-stack-up\n</code></pre> <pre><code>docker compose --file docker/docker-compose.yml up -d\n</code></pre> <p>You can access your local Airflow webserver console via https://localhost:8443.</p> <p>Note</p> <p>It is safe to ignore the certificate error exceptions raised by your browser as the stack is using a self-signed certificate.</p> <p>Info</p> <p>Check the credentials management for instructions on Airflow webserver authentication.</p>"},{"location":"installation-modes/celeryexecutor/instance-runtime/#stop","title":"Stop","text":"MakesterDocker CLI <pre><code>make celery-stack-down\n</code></pre> <pre><code>docker compose --file docker/docker-compose.yml down\n</code></pre>"},{"location":"installation-modes/celeryexecutor/instance-runtime/#docker-files","title":"Docker files","text":"<p>Flowz Apache Airflow in CeleryExecutor mode is delivered as a containerised service. Use Docker <code>compose</code> to standup the services.</p>"},{"location":"installation-modes/celeryexecutor/instance-runtime/#configuration","title":"Configuration","text":"MakesterDocker CLI <pre><code>make celery-stack-config\n</code></pre> <pre><code>docker compose --file docker/docker-compose.yml config\n</code></pre> <p>The environment variables are fed into the <code>docker-compose.yml</code>: Apache Airflow CeleryExecutor Docker compose file.<pre><code>---\n\nx-airflow-common: &amp;airflow-common\n    AIRFLOW_HOME: ${FLOWZ_AIRFLOW_HOME:-/opt/airflow/airflow}\n    AIRFLOW_CUSTOM_ENV: ${FLOWZ_AIRFLOW_CUSTOM_ENV:-dev}\n    AIRFLOW_CUSTOM_CONFIG:\n        ${FLOWZ_CUSTOM_CONFIG:-/home/airflow/.local/lib/python3.12/site-packages/flowz/config}\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__DAGS_FOLDER:\n        ${AIRFLOW__CORE__DAGS_FOLDER:-/home/airflow/.local/lib/python3.12/site-packages/flowz/dags}\n    AIRFLOW__CORE__PLUGINS_FOLDER:\n        ${AIRFLOW__CORE__PLUGINS_FOLDER:-/home/airflow/.local/lib/python3.12/site-packages/flowz/plugins}\n    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES:-False}\n    AIRFLOW__CORE__UNIT_TEST_MODE: ${AIRFLOW__CORE__UNIT_TEST_MODE:-False}\n    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS:-True}\n    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL:\n        ${AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL:-30}\n    AIRFLOW__CORE__FERNET_KEY:\n        ${AIRFLOW__CORE__FERNET_KEY:-\"LFKF4PSrAOG-kbxOouoLj8Du2QCnsp9qw7G21-WPsLU=\"}\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:\n        ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql://airflow:airflow@postgres:5432/airflow?}\n    AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS:-False}\n    AIRFLOW__DATABASE__CHECK_MIGRATIONS: ${AIRFLOW__DATABASE__CHECK_MIGRATIONS:-False}\n    AIRFLOW__WEBSERVER__DAG_DEFAULT_VIEW: ${AIRFLOW__WEBSERVER__DAG_DEFAULT_VIEW:-graph}\n    AIRFLOW__WEBSERVER__WEB_SERVER_PORT: ${AIRFLOW__WEBSERVER__WEB_SERVER_PORT:-8443}\n    AIRFLOW__WEBSERVER__WORKERS: ${AIRFLOW__WEBSERVER__WORKERS:-1}\n    AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE:\n        ${AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE:-1}\n    AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT:\n        ${AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT:-/etc/ssl/certs/airflow-selfsigned.crt}\n    AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY:\n        ${AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY:-/home/airflow/ssl/private/airflow-selfsigned.key}\n    AIRFLOW__SCHEDULER__PRINT_STATS_INTERVAL: ${AIRFLOW__SCHEDULER__PRINT_STATS_INTERVAL:-300}\n    AIRFLOW__SCHEDULER__SCHEDULER_IDLE_SLEEP_TIME: ${AIRFLOW__SCHEDULER__SCHEDULER_IDLE_SLEEP_TIME:-5}\n    AIRFLOW__LOGGING__BASE_LOG_FOLDER: ${AIRFLOW__LOGGING__BASE_LOG_FOLDER:-/var/log/airflow}\n    AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW__LOGGING__REMOTE_LOGGING:-False}\n\nservices:\n    redis:\n        image: redis:7.2-alpine\n        container_name: airflow-redis\n        entrypoint: [\"redis-server\", \"--protected-mode\", \"no\"]\n        environment:\n            REDIS_PASSWORD: ${REDIS_PASSWORD:-redispass}\n            REDIS_PROTO: ${REDIS_PROTO:-\"redis://\"}\n            REDIS_HOST: ${REDIS_HOST:-redis}\n            REDIS_PORT: ${REDIS_PORT:-6379}\n            REDIS_DBNUM: ${REDIS_DBNUM:-1}\n            AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL:-\"redis://redis:6379/1\"}\n            AIRFLOW__CELERY__RESULT_BACKEND:\n                ${AIRFLOW__CELERY__RESULT_BACKEND:-db+postgresql://${POSTGRES_USER:-airflow}:${POSTGRES_USER:-airflow}@postgres:5432/airflow?}\n        healthcheck:\n            test: [\"CMD-SHELL\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\n            interval: 2s\n            timeout: 5s\n            retries: 2\n\n    postgres:\n        image: postgres:15.4-alpine\n        container_name: airflow-postgres\n        environment:\n            POSTGRES_USER: ${POSTGRES_USER:-airflow}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}\n            POSTGRES_HOST: ${POSTGRES_HOST:-postgres}\n            POSTGRES_PORT: ${POSTGRES_PORT:-5432}\n            POSTGRES_DB: ${POSTGRES_DB:-airflow}\n            POSTGRES_EXTRAS: ${POSTGRES_EXTRAS:-}\n            PGDATA: ${PGDATA:-/var/lib/postgresql/data/pgdata/db-files}\n        volumes:\n            - airflow-postgres-vol:/var/lib/postgresql/data/pgdata\n        healthcheck:\n            test: [\n                \"CMD-SHELL\",\n                \"pg_isready -d $${POSTGRES_DB:-airflow} -U $${POSTGRES_USER:-airflow}\"\n            ]\n            interval: 2s\n            timeout: 5s\n            retries: 2\n\n    init-db:\n        image: loum/flowz:latest\n        container_name: airflow-initdb\n        depends_on:\n            postgres:\n                condition: service_healthy\n        environment:\n            &lt;&lt;: *airflow-common\n        restart: \"no\"\n        command: db migrate\n        healthcheck:\n            test: [\"CMD\", \"db check\"]\n            interval: 2s\n            timeout: 5s\n            retries: 2\n\n    scheduler:\n        image: loum/flowz:latest\n        container_name: airflow-scheduler\n        hostname: scheduler\n        depends_on:\n            init-db:\n                condition: service_completed_successfully\n            redis:\n                condition: service_started\n        environment:\n            &lt;&lt;: *airflow-common\n        volumes:\n            - airflow-logs-vol:/var/log/airflow\n        restart: always\n        command: scheduler\n\n    webserver:\n        image: loum/flowz:latest\n        container_name: airflow-webserver\n        hostname: webserver\n        depends_on:\n            init-db:\n                condition: service_completed_successfully\n        environment:\n            &lt;&lt;: *airflow-common\n        volumes:\n            - airflow-logs-vol:/var/log/airflow\n        ports:\n            - 8443:8443\n        restart: always\n        command: webserver\n\n    worker:\n        image: loum/flowz:latest\n        container_name: airflow-worker\n        hostname: worker\n        extra_hosts:\n            host.docker.internal: host-gateway\n        depends_on:\n            init-db:\n                condition: service_completed_successfully\n            redis:\n                condition: service_started\n        environment:\n            &lt;&lt;: *airflow-common\n            FLOWZ_AIRFLOW_ADMIN_USER: ${FLOWZ_AIRFLOW_ADMIN_USER:-airflow}\n            FLOWZ_AIRFLOW_ADMIN_PASSWORD: ${FLOWZ_AIRFLOW_ADMIN_PASSWORD:-airflow}\n        volumes:\n            - airflow-worker-vol:/opt/airflow/data\n            - airflow-logs-vol:/var/log/airflow\n        restart: always\n        command: celery worker\n\n    minio:\n        image: quay.io/minio/minio\n        container_name: airflow-s3\n        hostname: minio\n        volumes:\n            - airflow-s3-vol:/data\n        ports:\n            - 9000:9000\n            - 9001:9001\n        command: server /data --console-address \":9001\"\n\nvolumes:\n    airflow-logs-vol:\n    airflow-worker-vol:\n    airflow-postgres-vol:\n    airflow-s3-vol:\n</code></pre></p>"},{"location":"installation-modes/celeryexecutor/instance-runtime/#flowz-default-container-image","title":"Flowz default container image","text":"<p>Use the following <code>shell</code> snippet if you are only interested in sampling the default Flowz container image:</p> <pre><code>curl -s https://raw.githubusercontent.com/loum/flowz/main/docker/docker-compose.yml |\\\n docker compose -f - up\n</code></pre> <p>For this simple demo usecase, the default Airflow webserver login credentials are <code>airflow</code>:<code>airflow</code>.</p> <p>This blocking variant of the <code>docker compose</code> can be stopped by hitting <code>Ctrl-C</code>.</p>"},{"location":"installation-modes/sequentialexecutor/dag-template/","title":"DAG starter template","text":"<p>Airflow DAGs are written in Python and are technically just a Python module (with <code>.py</code> extension). DAGs are interpreted by Airflow via the DagBag facility and can then be scheduled to execute.</p> <p>DAGs files are placed under the <code>AIRFLOW__CORE__DAGS_FOLDER</code>. The directory location can be identified as follows: <pre><code>make print-AIRFLOW__CORE__DAGS_FOLDER\n</code></pre></p> <p>The default DAG template can help you get started creating your new DAG. The template DAG at <code>src/flowz/dags/template.py</code> features a set of <code>start</code> and <code>end</code> \"book-end\" tasks that can be used to delimit your job. You then add your own business related tasks in between.</p> <p>The <code>start</code> and <code>end</code> tasks are instantiated via Airflow's EmptyOperators and act as safe landing zones for your job.</p> <p>Note</p> <p>More information around Airflow DAG creation and concepts is available at the Airflow tutorial.</p> <p>The actual source code of the <code>src/flowz/dags/template.py</code> is as follows: src/flowz/dags/template.py<pre><code>\"\"\"The simplest DAG template.\n\n\"\"\"\n\nfrom pathlib import PurePath\n\nimport airflow\n\nimport flowz.task\nfrom flowz.primer import Primer\n\n\nprimer = Primer(dag_name=PurePath(__file__).stem.replace(\"_\", \"-\"), department=\"ADMIN\")\nprimer.dag_properties.update(\n    {\"description\": \"Simple book-end DAG template to get you started\"}\n)\n\ndag = airflow.DAG(\n    primer.dag_id, default_args=primer.default_args, **(primer.dag_properties)\n)\n\ntask_start = flowz.task.start(dag, default_args=primer.default_args)\n#\n# Add your content here.\n#\ntask_end = flowz.task.end(dag, default_args=primer.default_args)\n\ntask_start &gt;&gt; task_end  # pylint: disable=pointless-statement\n</code></pre></p> <p>Copy <code>src/flowz/dags/template.py</code> into a new Python file. The filename should be descriptive enough to define your new workflow. In the following example, the Python module target will be called <code>sample</code>: <pre><code>cp src/flowz/dags/template.py src/flowz/dags/sample.py\n</code></pre></p> <p>A more detailed description about your new DAG can be provided by editing <code>src/flowz/dags/sample.py</code> and replacing the <code>description</code> variable value to suit. <code>description</code> renders in the Airflow UI and helps visitors understand the intent behind your workflow.</p> <p>A quick validation of your new DAG can be performed with:</p> MakesterAirflow CLI <pre><code>make local-list-dags\n</code></pre> <pre><code>venv/bin/airflow dags list\n</code></pre> Valid DAGs under the current runtime context.<pre><code>dag_id                | filepath     | owner   | paused\n======================+==============+=========+=======\nADMIN_BOOTSTRAP_LOCAL | bootstrap.py | airflow | False\nADMIN_SAMPLE_LOCAL    | sample.py    | airflow | True\n</code></pre> <p>The new sample DAG is rendered under the Apache Airflow dashboard's graph view as follows: </p>"},{"location":"installation-modes/sequentialexecutor/development-environment/","title":"Setup the development environment","text":"<p>Note</p> <p>At this point, it is assumed that you have already forked a copy of Flowz.</p>"},{"location":"installation-modes/sequentialexecutor/development-environment/#prerequisites","title":"Prerequisites","text":"<ul> <li>GNU make.</li> <li>Python 3 Interpreter. We recommend installing pyenv.</li> <li>Docker.</li> <li>To run local PySpark you will need OpenJDK 11.</li> </ul> <p>Note</p> <p>Makester is used as the Integrated Developer Platform.</p>"},{"location":"installation-modes/sequentialexecutor/development-environment/#macos-users-only-upgrading-gnu-make","title":"(macOS Users only) upgrading GNU <code>make</code>","text":"<p>Follow these notes to get GNU make.</p>"},{"location":"installation-modes/sequentialexecutor/development-environment/#build-the-local-virtual-environment","title":"Build the local virtual environment","text":"<p>Get the code and change into the top level <code>git</code> project directory. The following  uses the <code>flowz</code> project repository as an example: <pre><code>git clone https://github.com/loum/flowz.git &amp;&amp; cd flowz\n</code></pre></p> <p>Note</p> <p>Run all commands from the top-level directory of the <code>git</code> repository.</p> <p>For first-time setup, get the Makester project: <pre><code>git submodule update --init\n</code></pre></p> <p>Initialise the environment: <pre><code>make pristine\n</code></pre></p> <p>Run the test harness to validate your local environment setup: <pre><code>make tests\n</code></pre></p>"},{"location":"installation-modes/sequentialexecutor/flowz-conventions/","title":"Flowz conventions","text":"<p>Airflow as a workflow management system can be utilised as shared infrastructure between different teams and entities within the organisation. Having more contributors to the platform introduces a communal aspect where everyone can create and leverage existing code and tooling. However, as the number of DAGs begins to increase, so too could the maintenance burden. As such, it is recommended to adopt a set of development principles to ensure a degree of consistency within the framework. The following guidelines are a default recommendation.</p>"},{"location":"installation-modes/sequentialexecutor/flowz-conventions/#things-to-consider-when-creating-your-dags","title":"Things to consider when creating your DAGs","text":""},{"location":"installation-modes/sequentialexecutor/flowz-conventions/#distinction-between-dags-and-tasks","title":"Distinction between DAGs and tasks","text":"<p>A simple strategy that is utilised in Flowz is to capitalise DAG names and set task names to lower case.</p>"},{"location":"installation-modes/sequentialexecutor/flowz-conventions/#naming-standards","title":"Naming standards","text":"<p>The DAG name (Python module name) plays an integral part in the operation of Airflow. It is also the token that presents in the Airflow web UI.</p> <p>The DAG names are made up of three components separated by underscores (<code>_</code>):</p> <ol> <li>Department or team name (<code>department</code> parameter to <code>flowz.Primer</code>)</li> <li>Short name to give DAG some operational context (<code>dag_name</code> parameter to <code>flowz.Primer</code>)</li> <li>Environment is added automatically based on the setting of the environment variable <code>AIRFLOW_CUSTOM_ENV</code> (defaults to <code>local</code>)</li> </ol> <p>For example, the DAG name generated from the <code>src/flowz/dags/template.py</code> becomes <code>ADMIN_TEMPLATE_LOCAL</code></p> <p>Note</p> <p>Ensure the <code>dag_name</code> and <code>department</code> combination is unique amongst all DAGS under <code>AIRFLOW__CORE__DAGS_FOLDER</code> as this could cause an implicit conflict that is difficult to troubleshoot.</p>"},{"location":"installation-modes/sequentialexecutor/instance-runtime/","title":"Airflow instance runtime","text":"<p>Launch the Airflow webserver UI in SequentialExector mode to visualise and interact with dashboard (<code>Ctrl-C</code> to stop):</p> <p><pre><code>make local-airflow-start\n</code></pre> You can access your local Airflow webserver console via http://localhost:8888.</p> <p>The new sample DAG is rendered under the Apache Airflow dashboard's graph view as follows: </p> <p>Flowz pre-defines a set of Apache Airflow configuration items for a customised view. These are defined in the <code>resources/files/environment/local</code> file and can be edited to suit your preferences:</p> Apache Airflow customised configuration settings.<pre><code># airflow.cfg [core]\nAIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_HOME}/dags\nAIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_HOME}/plugins\nAIRFLOW__CORE__LOAD_EXAMPLES=False\n# Enable DAG Serialization\nAIRFLOW__CORE__STORE_SERIALIZED_DAGS=True\nAIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL=30\nAIRFLOW__CORE__STORE_DAG_CODE=True\n# Enable DAGs at startup\nAIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True\n# Quieten the webserver UI warnings\nAIRFLOW__CORE__UNIT_TEST_MODE=True\nAIRFLOW__CORE__EXECUTOR=SequentialExecutor\n\n# airflow.cfg [webserver]\nAIRFLOW__WEBSERVER__DAG_DEFAULT_VIEW=graph\nAIRFLOW__WEBSERVER__WEB_SERVER_PORT=8888\nAIRFLOW__WEBSERVER__WORKERS=1\n\n# airflow.cfg [logging]\nAIRFLOW__LOGGING__LOGGING_CONFIG_CLASS=flowz.config.logging.DEFAULT_LOGGING_CONFIG\nAIRFLOW__LOGGING__BASE_LOG_FOLDER=${AIRFLOW_HOME}/logs\nAIRFLOW__LOGGING__LOGGING_LEVEL=INFO\nAIRFLOW__LOGGING__LOG_FILENAME_TEMPLATE=dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index &gt;= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log\n\n# airflow.cfg [database]\nAIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:///${AIRFLOW_HOME}/airflow.db\n# Don't load default connections\nAIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False\n</code></pre>"},{"location":"installation-modes/sequentialexecutor/test-harness/","title":"Integrating DAGs into the test harness","text":"<p>Recall that the new <code>sample</code> DAG is just Python logic. As such, it can be validated by running the test harness:</p> <pre><code>make tests\n</code></pre> <p>Note that with the addition of the new <code>sample</code> DAG, Flowz has detected the change to the Airflow DagBag. The test harness will fail as a results:</p> New DAG fails the test harness.<pre><code>E       AssertionError: DagBag to \"DAG_TASK_IDS\" control list mis-match: check the DAG names defined by DAG_TASK_IDS in fixtures. Or, add to \"dag_names_to_skip\" in the test_dagbag_set() test to skip the check.\nE       assert ['ADMIN_BOOTSTRAP_LOCAL', 'ADMIN_SAMPLE_LOCAL']i == ['ADMIN_BOOTSTRAP_LOCAL']\nE\nE         Left contains one more item: 'ADMIN_SAMPLE_LOCAL'\nE\nE         Full diff:\nE           [\nE               'ADMIN_BOOTSTRAP_LOCAL',\nE         +     'ADMIN_SAMPLE_LOCAL',\nE           ]\n\ntests/flowz/dags/test_dags.py:35: AssertionError\n\n tests/flowz/dags/test_dags.py::test_dagbag_set \u2a2f\n</code></pre> <p>If the new DAG will form part of production deployments, then you may consider adding the appropriate coverage in the test. This way, the test harness will safeguard against syntastic errors and incorrect deletions. To do so, you will need to add an entry to the <code>DAG_TASK_IDS</code>:</p> DAG_TASK_IDS in tests/flowz/dags/conftest.py<pre><code>DAG_TASK_IDS = {\n    \"ADMIN_BOOTSTRAP_LOCAL\": [\n        \"end\",\n        \"load-connections\",\n        \"load-dag-variables\",\n        \"load-task-variables\",\n        \"set-authentication\",\n        \"start\",\n    ],\n}\n</code></pre> <p>Note that <code>DAG_TASK_IDS</code> is a dictionary based data structure that takes the DAG name as the key and the task names as values. Add the following to the <code>DAG_TASK_IDS</code>:</p> Adding new DAG to DAG_TASK_IDS<pre><code>DAG_TASK_IDS = {\n    \"ADMIN_BOOTSTRAP_LOCAL\": [\n        \"end\",\n        \"load-connections\",\n        \"load-dag-variables\",\n        \"load-task-variables\",\n        \"set-authentication\",\n        \"start\",\n    ],\n    \"ADMIN_SAMPLE_LOCAL\": [\n        \"end\"\n        \"start\",\n    ],\n}\n</code></pre> <p>Subsequent test harness passes should now complete successfully.</p> <p>Alternatively, you can skip the validation of the DAG in the test harness by adding the name of the DAG to the <code>dag_names_to_skip</code>variable in the test. This is an empty list by default as follows:</p> Skip DAG definition in the DagBag set validation.<pre><code>@unittest.mock.patch.dict(os.environ, {\"AIRFLOW_CUSTOM_ENV\": \"LOCAL\"})\ndef test_dagbag_set(\n    dag_names: Iterable[str],\n    dag_id_cntrl: KeysView,\n) -&gt; None:\n    \"\"\"Test the dagbag load.\"\"\"\n    # Given a list of DAG names taken from the DagBag\n    # dag_names\n\n    # less the DAG names that can be skipped from the check\n    dag_names_to_skip: list[str] = []\n    received = [x for x in dag_names if x not in dag_names_to_skip]\n\n    frame: FrameType | None = currentframe()\n    assert frame is not None\n    test_to_skip: str = frame.f_code.co_name\n    msg = (\n        'DagBag to \"DAG_TASK_IDS\" control list mis-match: '\n        \"check the DAG names defined by DAG_TASK_IDS in fixtures. \"\n        f'Or, add to \"dag_names_to_skip\" in the {test_to_skip}() '\n        \"test to skip the check.\"\n    )\n    expected = [x for x in dag_id_cntrl if x not in dag_names_to_skip]\n    assert sorted(received) == sorted(expected), msg\n</code></pre> <p>The following adjustment will suppress the DAG check: <pre><code>    ...\n    # less the DAG names that can be skipped from the check\n    dag_names_to_skip: list[str] = [\"ADMIN_SAMPLE_LOCAL\"]\n    ...\n</code></pre></p>"},{"location":"operations/bootstrap/auth/","title":"Airflow admin credentials","text":""},{"location":"operations/bootstrap/auth/#overriding-airflows-webserver_configpy","title":"Overriding Airflow's <code>webserver_config.py</code>","text":"<p>One of the key differences between the Flowz development environment and the remote executor variant (Celery Executor) is that Airflow webserver authentication is disabled during DAG development. Flowz achieves this by overriding Airflow's <code>webserver_config.py</code> during the initialisation of the development environment.</p>"},{"location":"operations/bootstrap/auth/#remote-executor-credentials-management","title":"Remote executor credentials management","text":"<p>Airflow remote executor installation types will enforce <code>admin</code> user authentication by default:</p> <p></p> <p>The Flowz bootstrap DAG features an <code>admin</code> user credentials primer task. By default, the username and password fields are both <code>airflow</code>.</p> <p>Warning</p> <p>Do not use the default primer credentials in a production environment.</p> <p>It is possible to override the default primer credentials by setting the environment variables <code>FLOWZ_AIRFLOW_ADMIN_USER</code> and <code>FLOWZ_AIRFLOW_ADMIN_PASSWORD</code> during the Airflow instance start up with values of your choice. For example:</p> <pre><code>FLOWZ_AIRFLOW_ADMIN_USER=user FLOWZ_AIRFLOW_ADMIN_PASSWORD=passwd make celery-stack-up\n</code></pre>"},{"location":"reference/flowz/connection/","title":"connection.py","text":"<p>Airflow connection helpers.</p>"},{"location":"reference/flowz/connection/#flowz.connection.delete_connection","title":"<code>delete_connection(key)</code>","text":"<p>Delete connection <code>key</code> from DB.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the Airflow Variable key.</p> required Source code in <code>flowz/connection.py</code> <pre><code>def delete_connection(key: str) -&gt; None:\n    \"\"\"Delete connection `key` from DB.\n\n    Parameters:\n        key: The name of the Airflow Variable key.\n\n    \"\"\"\n    log.info('Attempting to delete Airflow connection with conn_id: \"%s\"', key)\n    with LAZY_AF_UTILS.session.create_session() as session:  # type: ignore[operator,attr-defined]\n        try:\n            to_delete = (\n                session.query(LAZY_AF_MODELS.Connection)\n                .filter(LAZY_AF_MODELS.Connection.conn_id == key)  # type: ignore[attr-defined]\n                .one()\n            )\n        except exc.NoResultFound:\n            log.warning('Did not find a connection with conn_id: \"%s\"', key)\n        except exc.MultipleResultsFound:\n            log.warning('Found more than one connection with conn_id: \"%s\"', key)\n        else:\n            session.delete(to_delete)\n            log.info('Successfully deleted Airflow connection with conn_id: \"%s\"', key)\n</code></pre>"},{"location":"reference/flowz/connection/#flowz.connection.list_connections","title":"<code>list_connections()</code>","text":"<p>Return connection information from Airflow connections table.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list of all available connections.</p> Source code in <code>flowz/connection.py</code> <pre><code>def list_connections() -&gt; list[str]:\n    \"\"\"Return connection information from Airflow connections table.\n\n    Returns:\n        list of all available connections.\n\n    \"\"\"\n    with LAZY_AF_UTILS.session.create_session() as session:  # type: ignore[operator,attr-defined]\n        query = session.query(LAZY_AF_MODELS.Connection)\n        conns = query.all()\n\n        LAZY_AF_CLI_SIMPLE_TABLE.AirflowConsole().print_as(  # type: ignore[operator]\n            data=conns,\n            output=\"table\",\n            mapper=LAZY_AF_CONNECTION_COMMAND._connection_mapper,  # pylint: disable=protected-access\n        )\n\n    return [x.conn_id for x in conns]\n</code></pre>"},{"location":"reference/flowz/connection/#flowz.connection.set_connection","title":"<code>set_connection(path_to_connections)</code>","text":"<p>Add configuration items to Airflow <code>airflow.models.Connection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_connections</code> <code>str</code> <p>File path the the Airflow connections configuration.</p> required Source code in <code>flowz/connection.py</code> <pre><code>def set_connection(path_to_connections: str) -&gt; None:\n    \"\"\"Add configuration items to Airflow `airflow.models.Connection`.\n\n    Parameters:\n        path_to_connections: File path the the Airflow connections configuration.\n\n    \"\"\"\n    raw_connections = filester.get_directory_files(\n        path_to_connections, file_filter=\"*.json\"\n    )\n    log.info('Checking \"%s\" for Airflow connections ...', path_to_connections)\n    for raw_connection in raw_connections:\n        log.info('Found Airflow connection \"%s\"', raw_connection)\n        with open(\n            raw_connection,\n            encoding=\"utf-8\",\n        ) as json_file:\n            data = json.load(json_file)\n\n            conn_extra = data.pop(\"conn_extra\", None)\n            new_conn = LAZY_AF_MODELS.Connection(**data)  # type: ignore[operator]\n            if conn_extra:\n                new_conn.set_extra(json.dumps(conn_extra))\n\n            with LAZY_AF_UTILS.session.create_session() as session:  # type: ignore\n                state = \"OK\"\n                if (\n                    session.query(LAZY_AF_MODELS.Connection)\n                    .filter(\n                        LAZY_AF_MODELS.Connection.conn_id == new_conn.conn_id  # type: ignore\n                    )\n                    .first()\n                ):\n                    state = \"already exists\"\n                else:\n                    session.add(new_conn)\n\n                msg = f'Airflow connection \"{data.get(\"conn_id\")}\" create status'\n                log.info(\"%s: %s\", msg, state)\n</code></pre>"},{"location":"reference/flowz/connection/#flowz.connection.set_logging_connection","title":"<code>set_logging_connection(path_to_connections=None)</code>","text":"<p>Logging configuration to Airflow <code>airflow.models.Connection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_connections</code> <code>str | None</code> <p>Optional file path the the Airflow connections configuration.</p> <code>None</code> Source code in <code>flowz/connection.py</code> <pre><code>def set_logging_connection(path_to_connections: str | None = None) -&gt; None:\n    \"\"\"Logging configuration to Airflow `airflow.models.Connection`.\n\n    Parameters:\n        path_to_connections: Optional file path the the Airflow connections configuration.\n\n    \"\"\"\n    if REMOTE_LOGGING:\n        if not path_to_connections:\n            path_to_connections = str(\n                PurePath(Path(__file__).resolve().parents[0]).joinpath(\n                    \"config\",\n                    \"templates\",\n                    \"connections\",\n                    \"logging\",\n                    \"sas\",\n                )\n            )\n        set_templated_connection(path_to_connections)\n    else:\n        log.info('Remote logging not enabled. Check \"AIRFLOW__LOGGING__REMOTE_LOGGING\"')\n</code></pre>"},{"location":"reference/flowz/connection/#flowz.connection.set_templated_connection","title":"<code>set_templated_connection(path_to_connections, environment_override=None)</code>","text":"<p>Add configuration items to Airflow <code>airflow.models.Connection</code>.</p> <p>Connection templates are sourced from the <code>path_to_connections</code> directory and should feature a <code>*.j2</code> extension. Each template file should feature a single <code>airflow.models.Connection</code> definition in JSON format. For example: <pre><code>    {\n        \"conn_id\": \"azure_wasb_logs\",\n        \"conn_type\": \"wasb\",\n        \"login\": \"login\",\n        \"password\": \"secret\"\n    }\n</code></pre></p> <p>Connections that are defined in the environment path override, <code>environment_override</code>, will task precedence over the defaults settings.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_connections</code> <code>str</code> <p>File path the the Airflow connections configuration.</p> required <code>environment_override</code> <code>str | None</code> <p>Provide an environment value that overrides settings defined under <code>path_to_connections</code>.</p> <code>None</code> Source code in <code>flowz/connection.py</code> <pre><code>def set_templated_connection(\n    path_to_connections: str, environment_override: str | None = None\n) -&gt; None:\n    \"\"\"Add configuration items to Airflow `airflow.models.Connection`.\n\n    Connection templates are sourced from the `path_to_connections` directory and should feature\n    a `*.j2` extension. Each template file should feature a single\n    `airflow.models.Connection` definition in JSON format. For example:\n    ```\n        {\n            \"conn_id\": \"azure_wasb_logs\",\n            \"conn_type\": \"wasb\",\n            \"login\": \"login\",\n            \"password\": \"secret\"\n        }\n    ```\n\n    Connections that are defined in the environment path override, `environment_override`, will\n    task precedence over the defaults settings.\n\n    Parameters:\n        path_to_connections: File path the the Airflow connections configuration.\n        environment_override: Provide an environment value that overrides settings defined\n            under `path_to_connections`.\n\n    \"\"\"\n    config_paths = []\n    if environment_override is not None:\n        config_paths.append(\n            str(\n                PurePath(Path(path_to_connections)).joinpath(\n                    environment_override.lower()\n                )\n            )\n        )\n    config_paths.append(path_to_connections)\n\n    for config_path in config_paths:\n        for path_to_variable_template in filester.get_directory_files(\n            str(config_path), file_filter=\"*.j2\"\n        ):\n            rendered_content: str | None = build_from_template(\n                {}, path_to_variable_template, write_output=False\n            )\n            if rendered_content is None:\n                continue\n\n            data = json.loads(rendered_content)\n\n            conn_extra = data.pop(\"conn_extra\", None)\n            new_conn = LAZY_AF_MODELS.Connection(**data)  # type: ignore[operator]\n            if conn_extra:\n                new_conn.set_extra(json.dumps(conn_extra))\n\n            with LAZY_AF_UTILS.session.create_session() as session:  # type: ignore[attr-defined]\n                state = \"OK\"\n                if (\n                    session.query(LAZY_AF_MODELS.Connection)\n                    .filter(\n                        LAZY_AF_MODELS.Connection.conn_id == new_conn.conn_id  # type: ignore\n                    )\n                    .first()\n                ):\n                    state = \"already exists\"\n                else:\n                    session.add(new_conn)\n\n                msg = f'Airflow connection \"{data.get(\"conn_id\")}\" create status'\n                log.info(\"%s: %s\", msg, state)\n</code></pre>"},{"location":"reference/flowz/primer/","title":"primer.py","text":"<p>Primer provides consistent context for your program's workflows.</p> <ul> <li><code>dag_name</code> and <code>department</code> form the <code>dag_id</code> that presents in the Airflow dashboard.</li> <li><code>department</code> organisation/department/team delimiter to categorise DAG ID.</li> <li><code>airflow_env_variable</code> is the name used in the Airflow infrustructure   environment that determines the instance context. For example,   <code>local</code>, <code>development</code> and <code>production</code>. Environment naming rules are not enforced.</li> </ul> <p><code>kwargs</code> accepts parameters that are passed into <code>airflow.models.dag.DAG</code>.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer","title":"<code>Primer</code>","text":"<p>Common components that can get your DAGs running with minimal fuss.</p> Source code in <code>flowz/primer.py</code> <pre><code>class Primer:\n    \"\"\"Common components that can get your DAGs running with minimal fuss.\"\"\"\n\n    def __init__(\n        self,\n        dag_name: str,\n        department: str = \"dept\",\n        airflow_env_variable: str = \"AIRFLOW_CUSTOM_ENV\",\n    ):\n        self.__dag_name = dag_name\n        self.__department = department\n        self.__airflow_env_variable = airflow_env_variable\n        self.__default_args = {\n            \"owner\": \"airflow\",\n            \"depends_on_past\": False,\n            \"email_on_failure\": False,\n            \"email_on_retry\": False,\n            \"retries\": 2,\n            \"retry_delay\": datetime.timedelta(minutes=5),\n        }\n        self.__dag_properties = {\n            \"schedule_interval\": None,\n            \"start_date\": Primer.derive_start_date(),\n            \"catchup\": False,\n        }\n\n    @property\n    def dag_name(self) -&gt; str:\n        \"\"\"`dag_name` getter.\"\"\"\n        return self.__dag_name\n\n    @property\n    def department(self) -&gt; str:\n        \"\"\"`department` getter.\"\"\"\n        return self.__department\n\n    @property\n    def airflow_env_variable(self) -&gt; str:\n        \"\"\"`airflow_env_variable` getter.\"\"\"\n        return self.__airflow_env_variable\n\n    @property\n    def dag_id(self) -&gt; str:\n        \"\"\"`dag_id` getter.\"\"\"\n        return f\"{self.department}_{self.dag_name}_{self.get_env}\".upper()\n\n    @property\n    def default_args(self) -&gt; dict[str, Any]:\n        \"\"\"The DAG's Operator-specific default arguments.\"\"\"\n        return self.__default_args\n\n    @property\n    def dag_properties(self) -&gt; dict[Any, Any]:\n        \"\"\"Provide sane DAG parameter defaults.\"\"\"\n        return self.__dag_properties\n\n    @property\n    def get_env(self) -&gt; str:\n        \"\"\"Return current environement name.\"\"\"\n        return os.environ.get(self.airflow_env_variable, \"local\").upper()\n\n    @staticmethod\n    def derive_start_date(timezone: str = \"Australia/Melbourne\") -&gt; datetime.datetime:\n        \"\"\"Define the DAG start date.\n\n        Supported date formats are `%Y-%m-%d`.\n\n        If no date is identified in the `CONFIG` then date defaults\n        to the first day of the current year.\n\n        Parameters:\n            timezone: Timezone context.\n\n        Returns:\n            `datetime` object representing the DAG start date.\n\n        \"\"\"\n\n        def current_year() -&gt; int:\n            return datetime.datetime.now().year\n\n        local_tz = pendulum.timezone(timezone)  # type: ignore\n\n        _dt = datetime.datetime(current_year(), 1, 1, tzinfo=local_tz)\n\n        return _dt\n</code></pre>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.airflow_env_variable","title":"<code>airflow_env_variable: str</code>  <code>property</code>","text":"<p><code>airflow_env_variable</code> getter.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.dag_id","title":"<code>dag_id: str</code>  <code>property</code>","text":"<p><code>dag_id</code> getter.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.dag_name","title":"<code>dag_name: str</code>  <code>property</code>","text":"<p><code>dag_name</code> getter.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.dag_properties","title":"<code>dag_properties: dict[Any, Any]</code>  <code>property</code>","text":"<p>Provide sane DAG parameter defaults.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.default_args","title":"<code>default_args: dict[str, Any]</code>  <code>property</code>","text":"<p>The DAG's Operator-specific default arguments.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.department","title":"<code>department: str</code>  <code>property</code>","text":"<p><code>department</code> getter.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.get_env","title":"<code>get_env: str</code>  <code>property</code>","text":"<p>Return current environement name.</p>"},{"location":"reference/flowz/primer/#flowz.primer.Primer.derive_start_date","title":"<code>derive_start_date(timezone='Australia/Melbourne')</code>  <code>staticmethod</code>","text":"<p>Define the DAG start date.</p> <p>Supported date formats are <code>%Y-%m-%d</code>.</p> <p>If no date is identified in the <code>CONFIG</code> then date defaults to the first day of the current year.</p> <p>Parameters:</p> Name Type Description Default <code>timezone</code> <code>str</code> <p>Timezone context.</p> <code>'Australia/Melbourne'</code> <p>Returns:</p> Type Description <code>datetime</code> <p><code>datetime</code> object representing the DAG start date.</p> Source code in <code>flowz/primer.py</code> <pre><code>@staticmethod\ndef derive_start_date(timezone: str = \"Australia/Melbourne\") -&gt; datetime.datetime:\n    \"\"\"Define the DAG start date.\n\n    Supported date formats are `%Y-%m-%d`.\n\n    If no date is identified in the `CONFIG` then date defaults\n    to the first day of the current year.\n\n    Parameters:\n        timezone: Timezone context.\n\n    Returns:\n        `datetime` object representing the DAG start date.\n\n    \"\"\"\n\n    def current_year() -&gt; int:\n        return datetime.datetime.now().year\n\n    local_tz = pendulum.timezone(timezone)  # type: ignore\n\n    _dt = datetime.datetime(current_year(), 1, 1, tzinfo=local_tz)\n\n    return _dt\n</code></pre>"},{"location":"reference/flowz/templater/","title":"templater.py","text":"<p>Templating capability.</p>"},{"location":"reference/flowz/templater/#flowz.templater.build_from_template","title":"<code>build_from_template(env_map, template_file_path, write_output=False)</code>","text":"<p>Take <code>template_file_path</code> and template against variables defined by <code>env_map</code>.</p> <p><code>template_file_path</code> needs to end with a <code>.j2</code> extension as the generated content will be output to the <code>template_file_path</code> less the <code>.j2</code>.</p> <p>A special custom filter <code>env_override</code> is available to bypass <code>env_map</code> and source the environment for variable substitution. Use the custom filter <code>env_override</code> in your template as follows: <pre><code>\"test\" : {{ \"default\" | env_override('CUSTOM') }}\n</code></pre></p> <p>Provided an environment variable as been set: <pre><code>export CUSTOM=some_value\n</code></pre></p> <p>The template will render: <pre><code>some_value\n</code></pre></p> <p>Otherwise: <pre><code>default\n</code></pre></p> Source code in <code>flowz/templater.py</code> <pre><code>def build_from_template(\n    env_map: dict, template_file_path: str, write_output: bool = False\n) -&gt; str:\n    \"\"\"Take `template_file_path` and template against variables defined by `env_map`.\n\n    `template_file_path` needs to end with a `.j2` extension as the generated\n    content will be output to the `template_file_path` less the `.j2`.\n\n    A special custom filter `env_override` is available to bypass `env_map` and\n    source the environment for variable substitution. Use the custom filter\n    `env_override` in your template as follows:\n    ```\n    \"test\" : {{ \"default\" | env_override('CUSTOM') }}\n    ```\n\n    Provided an environment variable as been set:\n    ```\n    export CUSTOM=some_value\n    ```\n\n    The template will render:\n    ```\n    some_value\n    ```\n\n    Otherwise:\n    ```\n    default\n    ```\n\n    \"\"\"\n\n    def env_override(value: str, key: str) -&gt; str:\n        return os.getenv(key, value)\n\n    target_template_file_path = os.path.splitext(template_file_path)\n\n    output = \"\"\n    try:\n        file_loader = jinja2.FileSystemLoader(os.path.dirname(template_file_path))\n        j2_env = jinja2.Environment(autoescape=True, loader=file_loader)\n\n        j2_env.filters[\"env_override\"] = env_override\n        template = j2_env.get_template(os.path.basename(template_file_path))\n\n        output = template.render(**env_map)\n\n        if write_output:\n            with tempfile.NamedTemporaryFile() as out_fh:\n                out_fh.write(output.encode())\n                out_fh.flush()\n                shutil.copy(out_fh.name, target_template_file_path[0])\n                logging.info(\n                    'Templated file \"%s\" generated', target_template_file_path[0]\n                )\n    except jinja2.exceptions.TemplateNotFound as err:\n        logging.error('Skipping templating: TemplateNotFound \"%s\"', err)\n\n    return output\n</code></pre>"},{"location":"reference/flowz/user/","title":"user.py","text":"<p>Apache Airflow user account management.</p>"},{"location":"reference/flowz/user/#flowz.user.delete_airflow_user","title":"<code>delete_airflow_user(user)</code>","text":"<p>Remove user from RBAC.</p> Source code in <code>flowz/user.py</code> <pre><code>def delete_airflow_user(user: str) -&gt; None:\n    \"\"\"Remove user from RBAC.\"\"\"\n    log.info('Deleting user \"%s\"', user)\n\n    with LAZY_AF_APP_BUILDER.get_application_builder() as appbuilder:  # type: ignore\n        try:\n            matched_user = next(\n                u for u in appbuilder.sm.get_all_users() if u.username == user\n            )\n            appbuilder.sm.del_register_user(matched_user)\n        except StopIteration:\n            log.warning(\n                'Deleting user \"%s\" failed (ignore for pristine bootstrap)', user\n            )\n</code></pre>"},{"location":"reference/flowz/user/#flowz.user.list_airflow_users","title":"<code>list_airflow_users()</code>","text":"<p>List Airflow users.</p> Source code in <code>flowz/user.py</code> <pre><code>def list_airflow_users() -&gt; list[str]:\n    \"\"\"List Airflow users.\"\"\"\n    users = []\n    with LAZY_AF_APP_BUILDER.get_application_builder() as appbuilder:  # type: ignore\n        users.extend(appbuilder.sm.get_all_users())\n\n    return [x.username for x in users]\n</code></pre>"},{"location":"reference/flowz/user/#flowz.user.set_admin_user","title":"<code>set_admin_user(user, password)</code>","text":"<p>Add Admin user to Airflow.</p> Source code in <code>flowz/user.py</code> <pre><code>def set_admin_user(user: str, password: str) -&gt; str:\n    \"\"\"Add Admin user to Airflow.\"\"\"\n    log.info('Adding RBAC auth user \"%s\"', user)\n\n    with LAZY_AF_APP_BUILDER.get_application_builder() as appbuilder:  # type: ignore\n        role = appbuilder.sm.find_role(\"Admin\")\n        fields = {\n            \"role\": role,\n            \"username\": user,\n            \"password\": password,\n            \"email\": \"\",\n            \"first_name\": \"Airflow\",\n            \"last_name\": \"Admin\",\n        }\n        user = appbuilder.sm.add_user(**fields)\n        if user:\n            log.info(\"Admin user bootstrapped successfully\")\n        else:\n            log.warning('Adding user \"%s\" failed', user)\n\n    return user\n</code></pre>"},{"location":"reference/flowz/user/#flowz.user.set_authentication","title":"<code>set_authentication()</code>","text":"<p>Set the Airflow Admin/Superuser account.</p> Source code in <code>flowz/user.py</code> <pre><code>def set_authentication() -&gt; None:\n    \"\"\"Set the Airflow Admin/Superuser account.\"\"\"\n    airflow_user = os.environ.get(\"FLOWZ_AIRFLOW_ADMIN_USER\", \"airflow\")\n    airflow_passwd = os.environ.get(\"FLOWZ_AIRFLOW_ADMIN_PASSWORD\", \"airflow\")\n\n    delete_airflow_user(airflow_user)\n    set_admin_user(airflow_user, airflow_passwd)\n    list_airflow_users()\n</code></pre>"},{"location":"reference/flowz/variable/","title":"variable.py","text":"<p>Flowz Airflow variable helpers.</p>"},{"location":"reference/flowz/variable/#flowz.variable.del_variable_key","title":"<code>del_variable_key(key)</code>","text":"<p>Airflow Variable delete helper.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the Airflow Variable key.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the Airflow Variable key was successfully deleted. Otherwise <code>False</code>.</p> Source code in <code>flowz/variable.py</code> <pre><code>def del_variable_key(key: str) -&gt; bool:\n    \"\"\"Airflow Variable delete helper.\n\n    Parameters:\n        key: The name of the Airflow Variable key.\n\n    Returns:\n        `True` if the Airflow Variable key was successfully deleted. Otherwise `False`.\n\n    \"\"\"\n    status = False\n    log.info('Deleting variable \"%s\"', key)\n    status = LAZY_AF_MODELS.Variable.delete(key)  # type: ignore[attr-defined]\n    if not status:\n        log.warning('Variable \"%s\" delete failed', key)\n\n    return status == 1 or False\n</code></pre>"},{"location":"reference/flowz/variable/#flowz.variable.del_variables","title":"<code>del_variables(path_to_variables)</code>","text":"<p>Delete variable items from Airflow <code>airflow.models.Variable</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_variables</code> <code>str</code> <p>File path the the Airflow variable configuration.</p> required Source code in <code>flowz/variable.py</code> <pre><code>def del_variables(path_to_variables: str) -&gt; None:\n    \"\"\"Delete variable items from Airflow `airflow.models.Variable`.\n\n    Parameters:\n        path_to_variables: File path the the Airflow variable configuration.\n\n    \"\"\"\n    env_map: dict = ENV_FILE.get(RUN_CONTEXT, {})\n\n    for path_to_variable_template in filester.get_directory_files(\n        path_to_variables, file_filter=\"*.j2\"\n    ):\n        rendered_content: str | None = build_from_template(\n            env_map, path_to_variable_template, write_output=False\n        )\n        if rendered_content is None:\n            continue\n\n        data = json.loads(rendered_content)\n\n        for var_name in data.keys():\n            del_variable_key(var_name)\n</code></pre>"},{"location":"reference/flowz/variable/#flowz.variable.get_variable","title":"<code>get_variable(name)</code>","text":"<p>Display variable by a given <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Airflow Variable identifier.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>the JSON value as a Python <code>dict</code> else None.</p> Source code in <code>flowz/variable.py</code> <pre><code>def get_variable(name: str) -&gt; dict[str, Any]:\n    \"\"\"Display variable by a given `name`.\n\n    Parameters:\n        name: Airflow Variable identifier.\n\n    Returns:\n        the JSON value as a Python `dict` else None.\n\n    \"\"\"\n    return LAZY_AF_MODELS.Variable.get(  # type: ignore[attr-defined]\n        name, default_var=None, deserialize_json=True\n    )\n</code></pre>"},{"location":"reference/flowz/variable/#flowz.variable.list_variables","title":"<code>list_variables()</code>","text":"<p>list the variable items from Airflow <code>airflow.models.Variable</code>.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[str, int]]</code> <p>A generator-type object with each Airflow Variable returned by the query.</p> Source code in <code>flowz/variable.py</code> <pre><code>def list_variables() -&gt; Iterator[tuple[str, int]]:\n    \"\"\"list the variable items from Airflow `airflow.models.Variable`.\n\n    Returns:\n        A generator-type object with each Airflow Variable returned by the query.\n\n    \"\"\"\n    with LAZY_AF_UTILS.session.create_session() as session:  # type: ignore[attr-defined]\n        qry = session.query(LAZY_AF_MODELS.Variable).all()\n\n        data = json.JSONDecoder()\n        for var in qry:\n            try:\n                val = data.decode(var.val)\n            except Exception:  # pylint: disable=broad-except\n                val = var.val\n            yield val\n</code></pre>"},{"location":"reference/flowz/variable/#flowz.variable.set_variables","title":"<code>set_variables(path_to_variables, environment_override=None)</code>","text":"<p>Add variable items to Airflow <code>airflow.models.Variable</code>.</p> <p>Variables that are defined in the environment path override, <code>environment_override</code>, will task precedence over the defaults settings.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_variables</code> <code>str</code> <p>File path the the Airflow variable configuration.</p> required <code>environment_override</code> <code>str | None</code> <p>Provide an environment value that overrides settings defined under <code>path_to_connections</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of variables inserted.</p> Source code in <code>flowz/variable.py</code> <pre><code>def set_variables(\n    path_to_variables: str, environment_override: str | None = None\n) -&gt; int:\n    \"\"\"Add variable items to Airflow `airflow.models.Variable`.\n\n    Variables that are defined in the environment path override, `environment_override`, will\n    task precedence over the defaults settings.\n\n    Parameters:\n        path_to_variables: File path the the Airflow variable configuration.\n        environment_override: Provide an environment value that overrides settings defined\n            under `path_to_connections`.\n\n    Returns:\n        The number of variables inserted.\n\n    \"\"\"\n    env_map: dict = ENV_FILE.get(RUN_CONTEXT, {})\n\n    counter = 0\n    config_paths = []\n    if environment_override is not None:\n        config_paths.append(\n            PurePath(path_to_variables).joinpath(PurePath(environment_override.lower()))\n        )\n    config_paths.append(PurePath(path_to_variables))\n\n    for config_path in config_paths:\n        for path_to_variable_template in filester.get_directory_files(\n            str(config_path), file_filter=\"*.j2\"\n        ):\n            rendered_content: str | None = build_from_template(\n                env_map, path_to_variable_template, write_output=False\n            )\n            if rendered_content is None:\n                continue\n\n            data = json.loads(rendered_content)\n\n            for var_name, values in data.items():\n                if get_variable(var_name):\n                    log.info(\n                        'Inserting variable \"%s\" skipped: already exists', var_name\n                    )\n                else:\n                    log.info('Inserting variable \"%s\"', var_name)\n                    LAZY_AF_MODELS.Variable.set(  # type: ignore[attr-defined]\n                        var_name, json.dumps(values, indent=4)\n                    )\n                    counter += 1\n\n    return counter\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/","title":"bootstrap.py","text":"<p>Bootstrap takes care of Airflow instance startup dependencies.</p>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.config_path","title":"<code>config_path()</code>","text":"<p>Bootstrapper configuration path.</p> <p>Returns:</p> Type Description <code>str</code> <p>Python string representing the fully qualified path to the custom configuration.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>def config_path() -&gt; str:\n    \"\"\"Bootstrapper configuration path.\n\n    Returns:\n        Python string representing the fully qualified path to the custom configuration.\n\n    \"\"\"\n\n    def inner() -&gt; str:\n        return str(PurePath(Path(__file__).resolve().parents[1]).joinpath(\"config\"))\n\n    return inner()\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.dag_name","title":"<code>dag_name()</code>","text":"<p>Use the DAG module name as the default DAG name.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the Airflow DAG name.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>def dag_name() -&gt; str:\n    \"\"\"Use the DAG module name as the default DAG name.\n\n    Returns:\n        String representation of the Airflow DAG name.\n\n    \"\"\"\n\n    def inner() -&gt; str:\n        return str(PurePath(__file__).stem.replace(\"_\", \"-\"))\n\n    return inner()\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.dag_params","title":"<code>dag_params()</code>","text":"<p>Bootstrapper DAG level parameter initialisation.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Python dictionary of bootrapper parameters at the DAG level.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>def dag_params() -&gt; dict:\n    \"\"\"Bootstrapper DAG level parameter initialisation.\n\n    Returns:\n        Python dictionary of bootrapper parameters at the DAG level.\n\n    \"\"\"\n\n    def inner() -&gt; dict:\n        return {\n            \"tags\": [dag_name().upper()],\n            \"schedule_interval\": \"@once\",\n            \"is_paused_upon_creation\": False,\n        }\n\n    return inner()\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.load_auth","title":"<code>load_auth()</code>","text":"<p>Task wrapper around setting the Airflow Admin/Superuser account.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>@task(task_id=\"set-authentication\")\ndef load_auth() -&gt; None:\n    \"\"\"Task wrapper around setting the Airflow Admin/Superuser account.\"\"\"\n    return flowz.user.set_authentication()\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.load_connections","title":"<code>load_connections(path_to_connections, environment_override=None)</code>","text":"<p>Task wrapper to add configuration items to Airflow <code>airflow.models.Connection</code>.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>@task(task_id=\"load-connections\")\ndef load_connections(\n    path_to_connections: str, environment_override: str | None = None\n) -&gt; None:\n    \"\"\"Task wrapper to add configuration items to Airflow `airflow.models.Connection`.\"\"\"\n    return flowz.connection.set_templated_connection(\n        path_to_connections=path_to_connections,\n        environment_override=environment_override,\n    )\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.task_load_dag_variables","title":"<code>task_load_dag_variables(path_to_variables, environment_override=None)</code>","text":"<p>Task wrapper to add DAG variable items to Airflow <code>airflow.models.Variable</code>.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>@task(task_id=\"load-dag-variables\")\ndef task_load_dag_variables(\n    path_to_variables: str, environment_override: str | None = None\n) -&gt; int:\n    \"\"\"Task wrapper to add DAG variable items to Airflow `airflow.models.Variable`.\"\"\"\n    return flowz.variable.set_variables(\n        path_to_variables=path_to_variables,\n        environment_override=environment_override,\n    )\n</code></pre>"},{"location":"reference/flowz/dags/bootstrap/#flowz.dags.bootstrap.task_load_task_variables","title":"<code>task_load_task_variables(path_to_variables, environment_override=None)</code>","text":"<p>Task wrapper to add task variable items to Airflow <code>airflow.models.Variable</code>.</p> Source code in <code>flowz/dags/bootstrap.py</code> <pre><code>@task(task_id=\"load-task-variables\")\ndef task_load_task_variables(\n    path_to_variables: str, environment_override: str | None = None\n) -&gt; int:\n    \"\"\"Task wrapper to add task variable items to Airflow `airflow.models.Variable`.\"\"\"\n    return flowz.variable.set_variables(\n        path_to_variables=path_to_variables,\n        environment_override=environment_override,\n    )\n</code></pre>"}]}